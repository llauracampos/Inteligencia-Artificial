# -*- coding: utf-8 -*-
"""Questao_3_Prova_2_IA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189J2aCMCK62bSpTTxmklSmBiXbTl-edp
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import davies_bouldin_score
import seaborn as sb

# %matplotlib inline
# %pylab inline
plt.style.use('ggplot')

data = pd.read_csv("Mall_Customers.csv")
data

"""Foi realizado um pré-processamento dos dados, excluindo os atributos irrelevantes para a predição (Id do cliente), e realizando a transformação das variáveis categóricas em numéricas.

### **Pré - Processamento dos Dados**
"""

# Transformação de variáveis categóricas 

genre_transform = LabelEncoder()
data.iloc[:, 1] = genre_transform.fit_transform(data.iloc[:, 1])

# Exclusão dos atributos não utilizados

dataset = data.drop(['CustomerID'], axis=1) 

dataset.head()

"""Inicialmente foi aplicado o método K-Means, que é algoritmo de aprendizado não supervisionado (ou seja, que não precisa de inputs de confirmação externos) que avalia e clusteriza os dados de acordo com suas características. Inicialmente foi realizado o método do cotovelo, para decidir quais números de clusteres (K) seriam adotados, testando a variância dos dados em relação ao número de clusters. Desse modo, foram adotados K=2, K=3 e K=4. Os resultados de cada aplicação do K-Means serão analisados ao final da execução dos diferentes Ks.

### **K-Means**
"""

# K-Means

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10)
    kmeans.fit(dataset)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title("Método do Cotovelo")
plt.xlabel("Número de Clusters")
plt.ylabel("Soma dos Quadrados dentro do Cluster ")
plt.show()

# k-Means - K = 2

kmeans_2 = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10)
clusters_2 = kmeans_2.fit_predict(dataset)

dataset['Cluster'] = clusters_2
dataset

# Plot - K = 2

sb.pairplot(dataset,'Cluster')

# k-Means - K = 3

kmeans_3 = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10)
clusters_3 = kmeans_3.fit_predict(dataset)

dataset['Cluster'] = clusters_3
dataset

# Plot - K = 3

sb.pairplot(dataset,'Cluster')

# k-Means - K = 4

kmeans_4 = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 300, n_init = 10)
clusters_4 = kmeans_4.fit_predict(dataset)

dataset['Cluster'] = clusters_4
dataset

# Plot - K = 4

sb.pairplot(dataset,'Cluster')

"""Foi utilizado o índice Davies-Bouldin, que é uma das medidas de avaliação de algoritmos de clusterização. Esse índice é calculado como a similaridade média de cada cluster com um cluster mais semelhante a ele. Quanto menor for a semelhança média, melhor será a separação dos clusters e melhor será o resultado do agrupamento realizado. Como visualizado, o agrupamento com o melhor resultado foi o de K = 3, como apontado no método do cotovelo."""

# Análise

labels = kmeans_2.labels_
score_KMeans2 = davies_bouldin_score(dataset, labels)
print("K = 2: ", score_KMeans2)

labels = kmeans_3.labels_
score_KMeans3 = davies_bouldin_score(dataset, labels)
print("K = 3: ", score_KMeans3)

labels = kmeans_4.labels_
score_KMeans4 = davies_bouldin_score(dataset, labels)
print("K = 4: ", score_KMeans4)

"""Em seguida, foi realizada a clusterização hierárquica, que é um algoritmo que constrói a hierarquia de clusters. Esse algoritmo começa com todos os pontos de dados atribuídos a um cluster próprio e, em seguida, dois clusters mais próximos são mesclados no mesmo cluster. No final, esse algoritmo termina quando há apenas um único cluster. Para a visualização dos resultados do agrupamento hierárquico, foi utilizado um dendrograma. Para a comparação, foram utilizados os métodos de linkage: "Ward" e "Average". A observação dos resultados da Clusterização Hierárquica também será feita ao final do Notebook.

### **Clusterização Hierárquica**
"""

# Dendrograma

dendrogram(linkage(dataset))
plt.show()

# Linkage - Ward

dendrogram(linkage(dataset, 'ward'), truncate_mode='level', p=4)
plt.show()

# Clusterização Hierárquica (Ward) - N_Clusters = 2

hc_model_W_2 = AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters=2)
clusters_W_2 = hc_model_W_2.fit_predict(dataset)

dataset['Cluster'] = clusters_W_2
dataset

# Plot - N_Clusters = 2

sb.pairplot(dataset,'Cluster')

# Clusterização Hierárquica (Ward) - N_Clusters = 3

hc_model_W_3 = AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters=3)
clusters_W_3 = hc_model_W_3.fit_predict(dataset)

dataset['Cluster'] = clusters_W_3
dataset

# Plot - N_Clusters = 3

sb.pairplot(dataset,'Cluster')

# Clusterização Hierárquica (Ward) - N_Clusters = 4

hc_model_W_4 = AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters=4)
clusters_W_4 = hc_model_W_4.fit_predict(dataset)

dataset['Cluster'] = clusters_W_4
dataset

# Plot - N_Clusters = 4

sb.pairplot(dataset,'Cluster')

# Linkage - Average

dendrogram(linkage(dataset, 'average'), truncate_mode='level', p=4)
plt.show()

# Clusterização Hierárquica (Average) - N_Clusters = 2

hc_model_A_2 = AgglomerativeClustering(affinity='euclidean', linkage='average', n_clusters=2)
clusters_A_2 = hc_model_A_2.fit_predict(dataset)

dataset['Cluster'] = clusters_A_2
dataset

# Plot - N_Clusters = 2

sb.pairplot(dataset,'Cluster')

# Clusterização Hierárquica (Average) - N_Clusters = 3

hc_model_A_3 = AgglomerativeClustering(affinity='euclidean', linkage='average', n_clusters=3)
clusters_A_3 = hc_model_A_3.fit_predict(dataset)

dataset['Cluster'] = clusters_A_3
dataset

# Plot - N_Clusters = 3

sb.pairplot(dataset,'Cluster')

# Clusterização Hierárquica (Average) - N_Clusters = 4

hc_model_A_4 = AgglomerativeClustering(affinity='euclidean', linkage='average', n_clusters=4)
clusters_A_4 = hc_model_A_4.fit_predict(dataset)

dataset['Cluster'] = clusters_A_4
dataset

# Plot - N_Clusters = 4

sb.pairplot(dataset,'Cluster')

"""De forma análoga ao K-means, foi utilizado o índice Davies-Bouldin, para avaliação da clusterização. Como sabemos, quanto menor for a semelhança média, melhor será a separação dos clusters e melhor será o resultado do agrupamento realizado. Como visualizado, o agrupamento com o melhor resultado foi o de N = 4, para os dois métodos de Linkage."""

# Análise

labels = hc_model_W_2.labels_
score_W_2 = davies_bouldin_score(dataset, labels)
print("Ward - N = 2: ", score_W_2)

labels = hc_model_W_3.labels_
score_W_3 = davies_bouldin_score(dataset, labels)
print("Ward - N = 3: ", score_W_3)

labels = hc_model_W_4.labels_
score_W_4 = davies_bouldin_score(dataset, labels)
print("Ward - N = 4: ", score_W_4)

labels = hc_model_A_2.labels_
score_A_2 = davies_bouldin_score(dataset, labels)
print("Average - N = 2: ", score_A_2)

labels = hc_model_A_3.labels_
score_A_3 = davies_bouldin_score(dataset, labels)
print("Average - N = 3: ", score_A_3)

labels = hc_model_A_4.labels_
score_A_4 = davies_bouldin_score(dataset, labels)
print("Average - N = 4: ", score_A_4)